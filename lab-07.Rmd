---
title: "Lab 07 - Modelling course evaluations"
author: "Sean Wang"
date: "`r Sys.Date()`"
output: html_document
---

### Packages and Data

```{r load-packages, message=FALSE, echo=TRUE}
library(tidyverse)
library(tidymodels)

```


```{r read-data}
evals<-read.csv("data/evals.csv", row.names=1)
```


# Exercise 1: Exploratory Data Analysis

1.  Visualize the distribution of `score` in the dataframe `evals`.

```{r viz-score}
evals %>%
  ggplot(mapping = aes(x = score)) +
  geom_histogram()
```

skewed to the left

2.  Visualize and describe the relationship between `score` and `bty_avg` using `geom_point()` to represent the data. 

```{r scatterplot}
evals %>%
  ggplot(mapping = aes(x = score, y = bty_avg)) +
  geom_jitter() +
  geom_smooth()

evals %>%
  ggplot(mapping = aes(x = score, y = bty_avg)) +
  geom_point()

```

The jitter geom is a convenient shortcut for geom_point(position = "jitter"). It adds a small amount of random variation to the location of each point, and is a useful way of handling overplotting caused by discreteness in smaller datasets.

# Exercise 2: Simple Linear regression with a numerical predictor

1. Fit a linear model called `score_bty_fit` to predict average professor evaluation `score` from average beauty rating (`bty_avg`). Print the regression output using `tidy()`.

```{r fit-score_bty_fit, eval = FALSE}
# remove eval = FALSE from the code chunk options after filling in the blanks
score_bty_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ bty_avg, data = evals)
```

```{r tidy-score_bty_fit, eval = FALSE}
# remove eval = FALSE from the code chunk options after filling in the blanks
tidy()
```

*Add your linear model here. Don't worry too much about notation, you can use things like score-hat.*

2. Plot the data again using `geom_jitter()`, and add the regression line.

```{r viz-score_bty_fit,eval=FALSE}
evals %>%
  ggplot(mapping = aes(x = score, y = bty_avg)) +
  geom_jitter() +
  geom_smooth(method = "lm")
```

3. Interpret the slope of the linear model in context of the data.

an average change of 0.06663704 in bty_avg, when score is increase by 1.

4. Interpret the intercept of the linear model in context of the data. Comment on whether or not the intercept makes sense in this context.

when the score is 0, the bty_avg = 3.88

5. Determine the $R^2$ of the model and interpret it in the context of the data.

```{r R2, eval = FALSE}
# remove eval = FALSE from the code chunk options after filling in the blanks
glance(score_bty_fit)$r.squared
```

the relation between the two given variables are not strong enough

6. Make a plot of residuals vs. predicted values for the model above.

```{r viz-score_bty_fit-diagnostic, eval = FALSE}
# remove eval = FALSE from the code chunk options after filling in the blanks
score_bty_aug <- augment(score_bty_fit$fit)

ggplot(data = score_bty_aug, mapping = aes(x = .fitted, y = .resid)) +
  geom_jitter() +
  geom_hline(yintercept = 0, lty = "dashed") 

# not appropriate, since the residual is not evenly ditributed above and below the y=0 line.
```

# Exercise 3: Simple Linear regression with a categorical predictor

0. Look at the variable rank, and determine the frequency of each category level.

```{r}
# ... 
```

1. Fit a new linear model called `score_rank_fit` to predict average professor evaluation `score` based on `rank` of the professor.

```{r fit-score_rank_fit}
score_rank_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ rank, data = evals) %>%
  tidy()
  
```

*Add your narrative here.*

2. Fit a new linear model called `score_gender_fit` to predict average professor evaluation `score` based on `gender` of the professor. 

```{r fit-score_gender_fit}
score_gender_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ gender, data = evals) %>%
  tidy()
```

```{r score_gender_intercept, eval=FALSE}
# remove eval = FALSE from the code chunk options
score_gender_intercept <- tidy(score_gender_fit) %>% 
  filter(term == "(Intercept)") %>%
  select(estimate) %>%
  pull()
```

```{r score_gender_slope, eval=FALSE}
# remove eval = FALSE from the code chunk options
score_gender_slope <- tidy(score_gender_fit) %>% 
  filter(term == "gendermale") %>%
  select(estimate) %>%
  pull()
```

score_gender_intercept <- tidy(score_gender_fit)$estimate[1]

# Exercise 4: Multiple linear regression

1. Fit a multiple linear regression model, predicting average professor evaluation `score` based on average beauty rating (`bty_avg`) and `gender.`

```{r fit-score_bty_gender_fit}
score_bty_gender_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ bty_avg + gender, data = evals) %>%
  tidy()
```

*Add your narrative here.*

```{r eval = FALSE}
evals %>%
ggplot(mapping = aes(x = score, y = bty_avg)) + 
  geom_jitter(mapping = aes(color = gender))
```

2. What percent of the variability in `score` is explained by the model `score_bty_gender_fit`. 

```{r}
glance(score_bty_gender_fit)$r.squared

# 0.059
```


3. What is the equation of the line corresponding to just male professors?

y = 0.17238955 * x1 + 0.07415537 * x2 + 3.74733824

4. For two professors who received the same beauty rating, which gender tends to have the higher course evaluation score?

male

5. How does the relationship between beauty and evaluation score vary between male and female professors?

r^2 of male prof tends to be higher

6. How do the adjusted $R^2$ values of `score_bty_fit` and `score_bty_gender_fit` compare? 

```{r eval=FALSE}
# remove eval = FALSE from the code chunk options after filling in the blanks
glance(score_bty_fit)$adj.r.squared
glance(score_bty_gender_fit)$adj.r.squared
```

*Add your narrative here.*

7. Compare the slopes of `bty_avg` under the two models (`score_bty_fit` and `score_bty_gender_fit`).

*Add your narrative here.*

# Exercise 5: Interpretation of log-transformed response variables

If you do not know how to use LaTeX, do this exercise with pen and paper.
